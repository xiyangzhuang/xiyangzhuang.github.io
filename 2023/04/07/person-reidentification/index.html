<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tableau-zxy.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="计算机视觉研究中，利用行人重识别技术可以判断同一个人是否在另一个地方被另一个传感器捕捉到。计算机视觉领域的学者们形象地将针对特定行人的监控视频检索问题称为行人重识别。随着公共安全的要求和摄像头等传感器设备被大规模被应用到主题公园，学校，街道，超市等场景中，传统的依靠人工的方式进行行人识别跟踪，需要大量的人力资本投入。同时，考虑到门店的建设成本，针对门店场景的行人重识别方法应考虑到算法的实施成本，这">
<meta property="og:type" content="article">
<meta property="og:title" content="行人重识别文献研究">
<meta property="og:url" content="http://tableau-zxy.github.io/2023/04/07/person-reidentification/index.html">
<meta property="og:site_name" content="zhuangxy&#39;blog">
<meta property="og:description" content="计算机视觉研究中，利用行人重识别技术可以判断同一个人是否在另一个地方被另一个传感器捕捉到。计算机视觉领域的学者们形象地将针对特定行人的监控视频检索问题称为行人重识别。随着公共安全的要求和摄像头等传感器设备被大规模被应用到主题公园，学校，街道，超市等场景中，传统的依靠人工的方式进行行人识别跟踪，需要大量的人力资本投入。同时，考虑到门店的建设成本，针对门店场景的行人重识别方法应考虑到算法的实施成本，这">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-04-07T13:18:44.057Z">
<meta property="article:modified_time" content="2023-04-06T04:46:38.029Z">
<meta property="article:author" content="Zhuang xiyang">
<meta property="article:tag" content="study">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://tableau-zxy.github.io/2023/04/07/person-reidentification/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>行人重识别文献研究 | zhuangxy'blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">zhuangxy'blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">行动决定价值</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://tableau-zxy.github.io/2023/04/07/person-reidentification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhuang xiyang">
      <meta itemprop="description" content="think and do it">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zhuangxy'blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          行人重识别文献研究
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-07 21:18:44" itemprop="dateCreated datePublished" datetime="2023-04-07T21:18:44+08:00">2023-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-06 12:46:38" itemprop="dateModified" datetime="2023-04-06T12:46:38+08:00">2023-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/articles/" itemprop="url" rel="index"><span itemprop="name">articles</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/articles/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/04/07/person-reidentification/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/04/07/person-reidentification/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>计算机视觉研究中，利用行人重识别技术可以判断同一个人是否在另一个地方被另一个传感器捕捉到。计算机视觉领域的学者们形象地将针对特定行人的监控视频检索问题称为行人重识别。随着公共安全的要求和摄像头等传感器设备被大规模被应用到主题公园，学校，街道，超市等场景中，传统的依靠人工的方式进行行人识别跟踪，需要大量的人力资本投入。同时，考虑到门店的建设成本，针对门店场景的行人重识别方法应考虑到算法的实施成本，这些都对行人的识别与跟踪，特别是在行人信息的高效检索等领域提出了新的挑战。</p>
<p>从技术上讲，实际视频监控系统的行人重识别系统可分为三个模块：行人检测，行人跟踪和行人信息检索。前两个任务是独立的计算机视觉任务，所以大家主要的工作还是最后一个模块。在行人重识别的研究中前期主要是围绕特定应用，进行特征提取，然后针对小规模数据集展开应用研究。近年来，随着数据量的增长以及深度学习研究的进一步推进，深度学习在行人识别检索的应用中取得了一定的成果。在门店场景顾客检索去重的研究中，对行人重识别提出了更高的要求。本文主要对从方法上对行人重识别做出一个大致的整理。
<span id="more"></span></p>
<h1 id="行人重识别基本原理">行人重识别基本原理</h1>
<p>行人重识别模型主通过使用单张图像作为输入与N张图像(g)进行匹配，这些图像的特征可描述为<span
class="math display">\[\left\{g_{i} \right\}_{i
=1}^{N}\]</span>，这N张图像属于N个不同的个体（ID）。给定一个个体q，其ID号<span
class="math display">\[i^{*}\]</span>可以通过以下公式获得：</p>
<p><span class="math display">\[
i^{*} = \text{argmax}_{i \in 1,2,\ldots,N}\text{sim}\left( q,\ g_{i}
\right)
\]</span></p>
<p>其中<span class="math display">\[\text{sim}\left( q,\
g_{i}\right)\]</span>是行人相似度度量函数，相似度最大的那个个体特征对应的ID即为所求的<em>i</em></p>
<h1 id="行人重识别门店场景可行性分析">行人重识别门店场景可行性分析</h1>
<p>1、 基于人脸识别进行行人重识别在门店场景中的应用可行性分析</p>
<p>理论上可行，但是有两个原因导致人脸识别较难应用：首先，广泛存在后脑勺和侧脸的情况，做正脸的人脸识别难。其次，摄像头拍摄的像素可能不高，尤其是远景摄像头里面人脸截出来很可能都没有达到32x32的像素。所以人脸识别在实际线下门店场景中的应用受到限制。</p>
<p>2、 基于衣服颜色的行人重识别在门店场景中的应用可行性分析</p>
<p>在行人重识别的过程中，衣服颜色确实是行人重识别做出判断一个重要因素，但光靠颜色是不足的。首先，摄像头之间是有色差，并且会有光照的影响。其次，有撞衫（颜色相似）的情况发生，要找细节，但比如颜色直方图这种统计的特征就把细节给忽略了。在门店顾客行人的重识别研究中，光用颜色特征是难以达到50%的top1正确率。</p>
<p>3、 直接将图像检索指标套用在行人重识别研究中是否合适</p>
<p>在早期，行人重识别数据集是由两个摄像头采集的比如viper，每个query只有一个正确的retrieval目标。所以往往使用top1比较。但在近期，随着大数据集的提出，数据集中往往包含多个摄像头的多个正确目标。光使用top1的话，不能反应模型的真实能力。所以类似图像检索，重识别加入了mAP（mean
average
precision）作为衡量标准，将排名为1~到n的结果（top1~n）都考虑进去。</p>
<p>4、测试方法可行性分析</p>
<p>目前行人重识别主要有两种方法。第一种方法是通过输入一对行人，输出这对行人的相似度，然后再按照相似度进行排序。第二种方法是提取特征，再计算与其他人的欧式距离，然后再按距离排序。</p>
<p>第一种方法的优点是，判断两个人是不是一个人，简单的二分类（是／否）。但缺点是如果我们搜索库中有m张图片，那么与目标图片组成m对图片对。每一对都要进一次模型，估算相似度，这极大的增加了测试的时间。如果我们有n个query，那么我们要计算nm次相似度（而m往往很大）。第二种方法是，预先提取行人的特征，我们只要预先提好n+m次特征。之后只要比较就好了，比较特征可以简单的用矩阵乘法实现。</p>
<p>通过搜索目前的研究成果，两种方案都有在用，但在门店客流场景中平台计算能力有限，因此用特征来快速检索更接近实际中图像搜索的要求。</p>
<!-- more -->
<h1 id="国内外研究文献整理">国内外研究文献整理</h1>
<p>行人重识别问题中的图片来源于不同的摄像头，然而，由于不同摄像头所处的角度、光照等环境的影响，行人重识别问题具有以下几个特点：由于实际监控环境中，无法使用脸部的有效信息，所以，只能利用行人的外貌特征来进行识别；在不同摄像头中，由于尺度、光照和角度的变化，同一个行人的不同图片中，外貌特征会有一定程度的变化；由于行人姿势及摄像头角度的变化，在不同摄像头中，不同行人的外貌特征可能比同一个人的外貌特征更相似。</p>
<p>学者从数据源将行人重识别问题分为基于影像的行人重识别技术和基于视频的行人重识别两类方法。本文为了搜索适用于门店客流场景的行人识别检索方法，重点围绕了基于特征表示、基于距离度量，基于深度学习技术在行人重别中的应用研究分别展开了讨论。</p>
<h2 id="基于特征表示的方法">基于特征表示的方法</h2>
<p>在视频监控环境中，行人的外貌特征比较容易提取和表示。因此，同一行人的不同外貌特征具有一定的鲁棒性。</p>
<p>1）(Dong, Cristani et al.
2011)为了减少视角变化导致的外貌变化，Farenzena M,
etal提出通过基于人身体对称性的特征提取方法。首先通过一个预处理过程在人身体上划分头、躯干、腿部和左右对称中轴，然后提取除了头部以外的各区域的多种特征，包括累积颜色特征和纹理特征。并基于对称中轴对特征进行加权，越靠近中轴权值越高[1]。</p>
<ol start="2" type="1">
<li>(Farenzena, Bazzani et
al.2010)l提出类似的方法，将绘画结构应用于行人重识别。用一个自适应的身体外形结构来表示行人像，包括头、胸、大腿和小腿，然后提取每个部分的颜色特征进行精确匹配[2]。</li>
</ol>
<p>3）(Bazzani, Cristani et
al.2012)结合行人的全局和局部外貌特征进行重识别，首先根据行人在单摄像头下的连续运动提取多个关键帧图像，并用多帧图像的累积HSV颜色直方图表示全局特征；其次，在把人身体分割成上、下半身并去除头部区域后，提取各上、下半身多帧图像中频繁出现的块信息表示局部特征；最后加权融合全局和局部特征进行行人重识别[3]。</p>
<h2 id="基于距离度量学习的方法">基于距离度量学习的方法</h2>
<p>上述基于特征的方法都是使用标准距离（如曼哈顿距离、欧氏距离和巴氏距离等）进行相似性度量。然而同一身份行人在跨越多个无重叠区摄像头时，不同外貌特征受视角、光照等因素的影响不同。标准的距离度量方法平等的对待每一种特征，而不会摒弃那些独立使用时效果很差的特征。因此，研究者尝试通过距离学习的方法，获得一个新的距离度量空间，使得同一行人不同图像的距离小于不同人间的距离。距离学习方法一般在马氏（Mahalanobis）距离的基础上进行，通过学习一个投影矩阵，使得在投影空间中同类样本之间的距离较小，而不同类样本之间的距离较大。</p>
<p>1）(Xing, Ng et
al.2002)提出距离测度学习的问题，他在Mahalanobis距离的基础上，根据样本的类别，将具有相同类别标签的样本组成正样本对，不同类别标签的样本构成负样本对，然后利用这些样本对作为约束条件来训练得到一个Mahalanobis矩阵，从而使得最终的距离度量函数能够尽可能地满足所给定的约束条件。[4]</p>
<p>2）(Weinberger and Saul 2009)提出最大近邻分类间隔(large marginnearest
neighbor
classification,LMNN)的算法，其思想类似于支持向量机，即希望寻求一个分类超平面，使得该超平面与最靠近点的距离尽可能大。同样，LMNN希望通过投影后，数据的邻域内的同类点向内部紧缩，不同类点向外扩张，并且之间的间隔尽可能大。[5]</p>
<p>3）(Dikmen, Akbas et al. 2010)对
LMNN进行改进提出LMNN-R方法，其用所有样本点的平均近邻边界来代替
LMNN中不同样本点所采用的各自近邻边界，取得了比LMNN
方法更强的约束效果。[6]</p>
<p>4）(Zheng, Gong et
al.2011)提出概率相对距离比较的方法，在学习距离度量函数时考虑相对约束，与之前的同类距离尽可能小，不同类距离尽可能大的要求不同，其要求同类的距离小于不同类之间的距离。对每一个样本，选择一个同类样本和不同类样本形成3元组，在训练过程通过最小化不同类样本距离减去同类样本距离的和，得到满足约束的距离度量矩阵。[7]</p>
<h2 id="基于神经网络的方法">基于神经网络的方法</h2>
<p>下述的方法主要基于神经网络来实现的，在这里从学者们的研究角度将行人重识别的思路大致分为基于人体部分的特征匹配和特征表达的损失函数设计两个方面。</p>
<h3 id="基于人体部分的特征匹配">基于人体部分的特征匹配</h3>
<p>基于人体部分的特征匹配，顾名思义，就是将人体看做是几个部分的组合，然后一部分一部分来比较。常见方案是水平切条，就是将图像切为几个水平的条。由于人体身材往往差不多，所以可以用简单的水平条来做一一比较[8-11]。在领域中做匹配，采用的是一个正方形的领域[12]。另一个较新的方案是先在人体上检测部件（手，腿，躯干等等）再进行匹配，这样的话可以减少位置的误差，但可能引入检测部件的误差[11,
13]。</p>
<h3
id="基于学习特征表达的损失函数设计">基于学习特征表达的损失函数设计</h3>
<p>为了便于说明我们这里首先介绍一下常用的损失函数。其实行人重识别训练大体上就是基于四种损失函数（loss），距离损失函数（distance
loss），分类损失函数（identification loss），验证损失函数（verification
loss），基于属性分类的损失函数（attributes loss）。distance
loss是最常用的loss，就是用feature之间的距离来计算loss，同一个人的feature应该更近，不同人的feature要更远，那么用同一个人的距离减掉不同人的距离加上一个正常数和0取max实现，代表的有triplet
loss，improved triplet loss，quadruplet loss等；identification
loss就是classification
loss，每一个人作为一个类别，来训练分类问题，把某个全连接层作为feature来做行人重识别，一般identification
loss能够很快收敛，但是面对百万级，千万级的数据稍显吃力；verification
loss就是两张图来做二分类，判断是否为同一个人，这个作为辅助训练还行，作为主训练的loss计算资源消耗巨大，其中constrastive
loss 就是其中的一种。在caffe的孪生神经网络（siamese
network）中，其采用的损失函数是contrastive loss；attributes
loss就是对于不同的属性做分类，比如上身颜色，是否戴帽子之类的，这个是有用的，但是标注数据太过奢侈。</p>
<p>对学者们近年来常用损失函数统计如下：(Zheng, Yang et al.
2016)采用identification
loss，直接通过拿身份Label来做多类分类来进行行人重识别[14]。(Li, Zhao et
al. 2014, Yi, Lei et al. 2014, Ahmed, Jones et al.
2015)采用的Verification loss来进行的行人重识别研究[8, 10, 12]。(Ding,
Lin et al. 2015, Hao, Feng et al.2017, Hermans, Beyer et al.
2017)采用的triplet loss来进行通过距离度量来进行行人重识别[15-17]。</p>
<h1 id="结论">结论</h1>
<p>在行人重识别的研究过程中，基于深度学习的方法成为当前的研究热点。上文就行人重识别采用的方法做了一个简要的总结。在进行人重识别之前要对行人进行检测，检测过程中涉及到检测算子的阈值问题，当阈值设得过为严格时，则检测会发生遗漏，减少参与匹配计算的影像数量，造成目标包含不全；当阈值设得过为宽松时，则可能会有更多的背景包含进去。这两种结果对行人重识别的结果都不好。另外一个方面就是如何在视频流中定位到个体出现的位置，这个任务要比行人检测、跟踪、重识别简单，不要求有那么高的检测精度，只要能定位就行。</p>
<p>另外在视频中进行行人跟踪相对简单，虽然不可避免的是会存在错误。但是，人脸识别、颜色、非背景信息都有利于提高行人跟踪的准确性。在追踪的过程中，行人会有比较大的变化。运用这些序列图来对行人重识别方法进行训练能大大减少监督学习对于标注数据的依赖。</p>
<p>近几年，主流方法是将检测算法得到的人视作由于多个部分组成的整体，利用深度学习预先对各个部分进行特征提取，然后通过相似性度量来判断行人是否出现过以前的某个位置。</p>
<h1 id="附录">附录</h1>
<p>这里主要列出行人去重数据集，其中，VIPeRk， Shinpuhkan，PRID，
i-LIDS，3DPeS，CUHK01~03这些的数据源有不同的场景，且只适用于单一场景，不能用于普适场景的表达。但这些数据集为门店去重数据集的构建提供一个参考依据，整理统计数据集如下表1所示。</p>
<blockquote>
<p>表1 行人去重数据集统计</p>
</blockquote>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 9%" />
<col style="width: 13%" />
<col style="width: 9%" />
<col style="width: 23%" />
<col style="width: 10%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="header">
<th>数据集</th>
<th>个体数量</th>
<th>摄像头 数量</th>
<th>影像数量</th>
<th>标注方法</th>
<th>裁剪大小</th>
<th>多场景 捕捉</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>VIPeR (2007)</td>
<td><strong>632</strong></td>
<td><strong>2</strong></td>
<td><strong>1264</strong></td>
<td><strong>Hand</strong></td>
<td><strong>128X48</strong></td>
<td></td>
</tr>
<tr class="even">
<td>ETH1,2,3 (2007)</td>
<td><strong>148</strong></td>
<td><strong>1</strong></td>
<td><strong>8580</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>QMUL iLIDS (2009)</td>
<td><strong>119</strong></td>
<td><strong>2</strong></td>
<td><strong>476</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>GRID (2009)</td>
<td><strong>1025</strong></td>
<td><strong>8</strong></td>
<td><strong>1275</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>CAVIAR4ReID (2011)</td>
<td><strong>72</strong></td>
<td><strong>2</strong></td>
<td><strong>1220</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>3DPeS (2011)</td>
<td><strong>192</strong></td>
<td><strong>8</strong></td>
<td><strong>1011</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>PRID2011 (2011)</td>
<td><strong>934</strong></td>
<td><strong>2</strong></td>
<td><strong>24541</strong></td>
<td><strong>Hand</strong></td>
<td><strong>128X64</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>V47 (2011)</td>
<td><strong>47</strong></td>
<td><strong>2</strong></td>
<td><strong>752</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>WARD (2012)</td>
<td><strong>70</strong></td>
<td><strong>3</strong></td>
<td><strong>4786</strong></td>
<td><strong>Hand</strong></td>
<td><strong>128X48</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>SAIVT-Softbio (2012)</td>
<td><strong>152</strong></td>
<td><strong>8</strong></td>
<td><strong>64472</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>CUHK01 (2012)</td>
<td><strong>971</strong></td>
<td><strong>2</strong></td>
<td><strong>3884</strong></td>
<td><strong>Hand</strong></td>
<td><strong>160X60</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>CUHK02 (2013)</td>
<td><strong>1816</strong></td>
<td><strong>10(5-pairs)</strong></td>
<td><strong>7264</strong></td>
<td><strong>Hand</strong></td>
<td><strong>160X60</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>CUHK03 (2014)</td>
<td><strong>1467</strong></td>
<td><strong>10(5 pairs)</strong></td>
<td><strong>13164</strong></td>
<td><strong>Hand/DPM</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>RAiD (2014)</td>
<td><strong>43</strong></td>
<td><strong>4</strong></td>
<td><strong>6920</strong></td>
<td><strong>Hand</strong></td>
<td><strong>128X64</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>iLIDS-VID (2014)</td>
<td><strong>300</strong></td>
<td><strong>2</strong></td>
<td><strong>42495</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>MPR Drone (2014)</td>
<td><strong>84</strong></td>
<td><strong>1</strong></td>
<td></td>
<td><strong>Pyramid Features(ACF)</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>HDA Person Dataset (2014)</td>
<td><strong>53</strong></td>
<td><strong>13</strong></td>
<td><strong>2976</strong></td>
<td><strong>Hand/Pyramid Features(ACF)</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>Shinpuhkan Dataset (2014)</td>
<td><strong>24</strong></td>
<td><strong>16</strong></td>
<td></td>
<td><strong>Hand</strong></td>
<td><strong>128X48</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>CASIA Gait Database B (2015)</td>
<td><strong>124</strong></td>
<td><strong>11</strong></td>
<td></td>
<td><strong>Background subtraction</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>Market1501 (2015)</td>
<td><strong>1501</strong></td>
<td><strong>6</strong></td>
<td><strong>32217</strong></td>
<td><strong>Hand/DPM</strong></td>
<td><strong>128X64</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>PRW (2016)</td>
<td><strong>932</strong></td>
<td><strong>6</strong></td>
<td><strong>34304</strong></td>
<td><strong>Hand</strong></td>
<td><strong>vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>Large scale person search (2016)</td>
<td><strong>11934s</strong></td>
<td><strong>-</strong></td>
<td><strong>34574</strong></td>
<td><strong>Hand</strong></td>
<td><strong>vary</strong></td>
<td></td>
</tr>
<tr class="odd">
<td>MARS (2016)</td>
<td><strong>1261</strong></td>
<td><strong>6</strong></td>
<td><strong>1191003</strong></td>
<td><strong>DPM+GMMCP</strong></td>
<td><strong>256X128</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>DukeMTMC-reID (2017)</td>
<td><strong>1812</strong></td>
<td><strong>8</strong></td>
<td><strong>36441</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="odd">
<td>DukeMTMC4ReID (2017)</td>
<td><strong>1852</strong></td>
<td><strong>8</strong></td>
<td><strong>46261</strong></td>
<td><strong>Doppia</strong></td>
<td><strong>Vary</strong></td>
<td><strong>✔</strong></td>
</tr>
<tr class="even">
<td>Airport (2017)</td>
<td><strong>9651</strong></td>
<td><strong>6</strong></td>
<td><strong>39902</strong></td>
<td><strong>ACF</strong></td>
<td><strong>128X64</strong></td>
<td><strong>✔</strong></td>
</tr>
</tbody>
</table>
<p>参考文献</p>
<p>[1] DONG S C, CRISTANI M, STOPPA M, et al. Custom Pictorial
Structures for Re-identification; proceedings of the British Machine
Vision Conference, F, 2011 [C].</p>
<p>[2] FARENZENA M, BAZZANI L, PERINA A, et al. Person re-identification
by symmetry-driven accumulation of local features; proceedings of the
Computer Vision and Pattern Recognition, F, 2010 [C].</p>
<p>[3] BAZZANI L, CRISTANI M, PERINA A, et al. Multiple-shot person
re-identification by chromatic and epitomic analyses [J]. Pattern
Recognition Letters, 2012, 33(7): 898-903.</p>
<p>[4] XING E P, NG A Y, JORDAN M I, et al. Distance metric learning,
with application to clustering with side-information; proceedings of the
International Conference on Neural Information Processing Systems, F,
2002 [C].</p>
<p>[5] WEINBERGER K Q, SAUL L K. Distance Metric Learning for Large
Margin Nearest Neighbor Classification [M]. JMLR.org, 2009.</p>
<p>[6] DIKMEN M, AKBAS E, HUANG T S, et al. Pedestrian Recognition with
a Learned Metric [J]. Lecture Notes in Computer Science, 2010,
6495(501-12.</p>
<p>[7] ZHENG W S, GONG S, XIANG T. Person re-identification by
probabilistic relative distance comparison; proceedings of the Computer
Vision and Pattern Recognition, F, 2011 [C].</p>
<p>[8] LI W, ZHAO R, XIAO T, et al. DeepReID: Deep Filter Pairing Neural
Network for Person Re-identification; proceedings of the Computer Vision
and Pattern Recognition, F, 2014 [C].</p>
<p>[9] LIAO S, HU Y, ZHU X, et al. Person re-identification by Local
Maximal Occurrence representation and metric learning; proceedings of
the Computer Vision and Pattern Recognition, F, 2015 [C].</p>
<p>[10] YI D, LEI Z, LIAO S, et al. Deep Metric Learning for Person
Re-identification; proceedings of the International Conference on
Pattern Recognition, F, 2014 [C].</p>
<p>[11] ZHAO R, OUYANG W, WANG X. Person Re-identification by Salience
Matching; proceedings of the IEEE International Conference on Computer
Vision, F, 2013[C].</p>
<p>[12] AHMED E, JONES M, MARKS T K. An improved deep learning
architecture for person re-identification; proceedings of the Computer
Vision and Pattern Recognition, F, 2015 [C].</p>
<p>[13] ZHENG L, HUANG Y, LU H, et al. Pose Invariant Embedding for Deep
Person Re-identification [J]. 2017.</p>
<p>[14] ZHENG L, YANG Y, HAUPTMANN A G. Person Re-dentification: Past,
Present and Future [J]. 2016,</p>
<p>[15] HAO L, FENG J, QI M, et al. End-to-End Comparative Attention
Networks for Person Re-Identification [J]. IEEE Transactions on Image
Processing A Publication of the IEEE Signal Processing Society, 2017,
26(7): 3492-506.</p>
<p>[16] HERMANS A, BEYER L, LEIBE B. In Defense of the Triplet Loss for
Person Re-Identification [J]. 2017,</p>
<p>[17] DING S, LIN L, WANG G, et al. Deep feature learning with
relative distance comparison for person re-identification [J]. Pattern
Recognition, 2015, 48(10):2993-3003.</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Zhuang xiyang 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Zhuang xiyang 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Zhuang xiyang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://tableau-zxy.github.io/2023/04/07/person-reidentification/" title="行人重识别文献研究">http://tableau-zxy.github.io/2023/04/07/person-reidentification/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
   <li class="post-copyright-license">
   并保留本声明。感谢您的阅读和支持！
  </li>

</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/study/" rel="tag"> <i class="fa fa-tag"></i> study</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/04/07/how-to-build-gui-with-python/" rel="prev" title="How to build gui with python">
      <i class="fa fa-chevron-left"></i> How to build gui with python
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/04/07/statistic-function/" rel="next" title="常见统计函数">
      常见统计函数 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">行人重识别基本原理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB%E9%97%A8%E5%BA%97%E5%9C%BA%E6%99%AF%E5%8F%AF%E8%A1%8C%E6%80%A7%E5%88%86%E6%9E%90"><span class="nav-number">2.</span> <span class="nav-text">行人重识别门店场景可行性分析</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%BD%E5%86%85%E5%A4%96%E7%A0%94%E7%A9%B6%E6%96%87%E7%8C%AE%E6%95%B4%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">国内外研究文献整理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.</span> <span class="nav-text">基于特征表示的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">基于距离度量学习的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.3.</span> <span class="nav-text">基于神经网络的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BA%BA%E4%BD%93%E9%83%A8%E5%88%86%E7%9A%84%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D"><span class="nav-number">3.3.1.</span> <span class="nav-text">基于人体部分的特征匹配</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E8%A1%A8%E8%BE%BE%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.3.2.</span> <span class="nav-text">基于学习特征表达的损失函数设计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">4.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">5.</span> <span class="nav-text">附录</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhuang xiyang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhuang xiyang</p>
  <div class="site-description" itemprop="description">think and do it</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/images/wechat_communication.png" title="WeChat → &#x2F;images&#x2F;wechat_communication.png"><i class="fab fa-weixin fa-fw"></i>WeChat</a>
      </span>
      <span class="links-of-author-item">
        <a href="/images/qq_communication.png" title="QQ → &#x2F;images&#x2F;qq_communication.png"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhuangxiyang@outlook.com" title="E-Mail → mailto:zhuangxiyang@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/tableau-zxy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;tableau-zxy" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhuang xiyang</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>-->
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共8k字</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'PSi7FY8PEMCGBOrsGIimN9Nd-gzGzoHsz',
      appKey     : 'I3MwowbOOP6US7iQbyl2nyio',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
